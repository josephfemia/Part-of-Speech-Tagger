{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_path = os.path.join(\"data\", \"brown-universal.txt\")\n",
    "tags_path = os.path.join(\"data\", \"tags-universal.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subset(object):\n",
    "    def __init__(self, full_dataset, keys, tags):\n",
    "        self.full_dataset = full_dataset\n",
    "        self.dataset = {}\n",
    "        self.keys = keys\n",
    "        self.tags = tags\n",
    "        self.vocab = []\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        \n",
    "        self._clean_up()\n",
    "        self._generate_vocab()\n",
    "        \n",
    "    def _clean_up(self):\n",
    "        for key in self.keys:\n",
    "            self.dataset[key] = self.full_dataset[key]\n",
    "            self.X.append(self.dataset[key]['sentence_words'])\n",
    "            self.Y.append(self.dataset[key]['sentence_tags'])\n",
    "        del self.full_dataset\n",
    "        \n",
    "    def _generate_vocab(self):\n",
    "        for sentence in self.dataset.values():\n",
    "            for word in sentence['sentence_words']:\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.append(word)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx) == int:\n",
    "            key = self.keys[idx]\n",
    "            return self.dataset[key]\n",
    "        \n",
    "        assert type(idx) == str and idx in self.keys\n",
    "        return self.dataset[idx]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        idx = 0\n",
    "        while(idx < len(self.dataset)):\n",
    "            yield self[idx]\n",
    "            idx += 1\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, brown_path, tags_path, train_test_split=0.8, seed=28934897):\n",
    "        self.brown_path = brown_path\n",
    "        self.tags_path = tags_path\n",
    "        self.train_test_split = train_test_split\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.tags = []\n",
    "        self.dataset = {}\n",
    "        self.keys = []\n",
    "        \n",
    "        self._prepare_tags()\n",
    "        self._prepare_brown()\n",
    "        self.datasets = self._split_dataset()\n",
    "        \n",
    "    def _prepare_tags(self):\n",
    "        with open(self.tags_path) as f:\n",
    "            raw_tag_data = f.read()\n",
    "            self.tags = raw_tag_data.lower().split('\\n')\n",
    "    \n",
    "    def _prepare_brown(self):\n",
    "        key = ''\n",
    "        \n",
    "        with open(brown_path) as f:\n",
    "            while(raw_data := f.readline()):\n",
    "                raw_data = raw_data.replace('\\n', '').replace('\\r', '').lower()\n",
    "                if len(raw_data) != 0:\n",
    "                    if raw_data.split('\\t')[-1] not in self.tags:\n",
    "                        key = raw_data\n",
    "                        self.dataset[key] = {'sentence_words': [], 'sentence_tags': []}\n",
    "                        self.keys.append(key)\n",
    "                    else:\n",
    "                        word_tag_pairing = raw_data.split('\\t')\n",
    "                        self.dataset[key]['sentence_words'].append(word_tag_pairing[0])\n",
    "                        self.dataset[key]['sentence_tags'].append(word_tag_pairing[1])\n",
    "                                                \n",
    "    def _split_dataset(self):\n",
    "        if self.seed:\n",
    "            random.seed(self.seed)\n",
    "        _keys = self.keys\n",
    "        random.shuffle(_keys)\n",
    "        split = int(self.train_test_split * len(_keys))\n",
    "        \n",
    "        self.training_dataset = Subset(self.dataset, _keys[:split], self.tags)\n",
    "        self.testing_dataset = Subset(self.dataset, _keys[split:], self.tags)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx) == int:\n",
    "            key = self.keys[idx]\n",
    "            return self.dataset[key]\n",
    "        \n",
    "        assert type(idx) == str and idx in self.keys\n",
    "        return self.dataset[idx]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        idx = 0\n",
    "        while(idx < len(self.dataset)):\n",
    "            yield self[idx]\n",
    "            idx += 1\n",
    "    \n",
    "data = Dataset(brown_path=brown_path, tags_path=tags_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostFrequentTagger(object):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.freq_counter = self._frequency_counter(dataset.X, dataset.Y)\n",
    "        self.table = {}\n",
    "        \n",
    "        self._generate_table()\n",
    "    \n",
    "    def _frequency_counter(self, seq_A, seq_B):\n",
    "        counter = defaultdict(Counter)\n",
    "        \n",
    "        for i in range(len(seq_A)):\n",
    "            for a, b in zip(seq_A[i], seq_B[i]):\n",
    "                counter[a][b] += 1\n",
    "                \n",
    "        return counter\n",
    "    \n",
    "    def _generate_table(self):\n",
    "        for word, tags in self.freq_counter.items():\n",
    "            self.table[word] = tags.most_common(1)[0][0]\n",
    "            \n",
    "    def predict(self, sentence):\n",
    "        result = [\"<start>\"]\n",
    "        \n",
    "        for word in sentence:\n",
    "            if pos := self.table.get(word):\n",
    "                result.append(pos)\n",
    "            else:\n",
    "                result.append('nan')\n",
    "        result.append('<end>')\n",
    "        \n",
    "        return result\n",
    "    \n",
    "mft = MostFrequentTagger(data.training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(dataset, model):\n",
    "    for key in dataset.keys[:3]:\n",
    "        print('---------------------------')\n",
    "        print('Sentence ID: ', key)\n",
    "\n",
    "        prediction = []\n",
    "        actual = []\n",
    "\n",
    "        sample = dataset[key]\n",
    "        print('Sentence: ', sample['sentence_words'])\n",
    "        print()\n",
    "        \n",
    "        prediction = model.predict(sample['sentence_words'])\n",
    "        actual += sample['sentence_tags']\n",
    "        actual.insert(0, '<start>')\n",
    "        actual.append('<end>')\n",
    "\n",
    "        print('Prediction: \\n\\r', prediction)\n",
    "        print('Actual: \\n\\r', actual)\n",
    "    print('---------------------------')\n",
    "    \n",
    "def calc_accuracy(X, Y, model):\n",
    "    correct = 0\n",
    "    total_tags = 0\n",
    "    \n",
    "    for sentence, actual_tags in zip(X, Y):\n",
    "        predicted_tags = model.predict(sentence)\n",
    "        for predicted_tag, actual_tag in zip(predicted_tags[1:-1], actual_tags):\n",
    "            if predicted_tag == actual_tag:\n",
    "                correct += 1\n",
    "            total_tags += 1\n",
    "            \n",
    "    return correct/total_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Sentence ID:  b100-49401\n",
      "Sentence:  ['``', \"i'll\", 'shore', 'be', 'needing', 'ye', 'both', 'on', 'the', 'pull', 'out', \"o'\", 'the', 'canyon', \"''\", '.']\n",
      "\n",
      "Prediction: \n",
      " ['<start>', '.', 'prt', 'noun', 'verb', 'verb', 'pron', 'det', 'adp', 'det', 'verb', 'prt', 'adp', 'det', 'noun', '.', '.', '<end>']\n",
      "Actual: \n",
      " ['<start>', '.', 'prt', 'noun', 'verb', 'verb', 'pron', 'det', 'adp', 'det', 'noun', 'prt', 'adp', 'det', 'noun', '.', '.', '<end>']\n",
      "---------------------------\n",
      "Sentence ID:  b100-18537\n",
      "Sentence:  ['she', 'named', '48', 'items', ',', 'and', 'said', 'there', 'were', '``', 'many', 'more', 'things', 'which', 'it', 'would', 'take', 'too', 'long', 'to', 'write', \"''\", '.']\n",
      "\n",
      "Prediction: \n",
      " ['<start>', 'pron', 'verb', 'num', 'noun', '.', 'conj', 'verb', 'prt', 'verb', '.', 'adj', 'adv', 'noun', 'det', 'pron', 'verb', 'verb', 'adv', 'adj', 'prt', 'verb', '.', '.', '<end>']\n",
      "Actual: \n",
      " ['<start>', 'pron', 'verb', 'num', 'noun', '.', 'conj', 'verb', 'prt', 'verb', '.', 'adj', 'adj', 'noun', 'det', 'pron', 'verb', 'verb', 'adv', 'adv', 'prt', 'verb', '.', '.', '<end>']\n",
      "---------------------------\n",
      "Sentence ID:  b100-35184\n",
      "Sentence:  [\"merton's\", 'functional', 'sociology', 'may', 'have', 'great', 'practical', 'use', 'in', 'the', 'study', 'of', 'different', 'cultures', ',', 'yet', 'it', 'is', 'perfectly', 'clear', ',', 'as', 'nagel', '(', '1957', ':', ':']\n",
      "\n",
      "Prediction: \n",
      " ['<start>', 'nan', 'adj', 'noun', 'verb', 'verb', 'adj', 'adj', 'noun', 'adp', 'det', 'noun', 'adp', 'adj', 'noun', '.', 'adv', 'pron', 'verb', 'adv', 'adj', '.', 'adp', 'nan', '.', 'num', '.', '.', '<end>']\n",
      "Actual: \n",
      " ['<start>', 'noun', 'adj', 'noun', 'verb', 'verb', 'adj', 'adj', 'noun', 'adp', 'det', 'noun', 'adp', 'adj', 'noun', '.', 'conj', 'pron', 'verb', 'adv', 'adj', '.', 'adp', 'noun', '.', 'num', '.', '.', '<end>']\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "visualize_predictions(data.testing_dataset, mft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 95.56%\n",
      "Testing Accuracy: 93.19%\n"
     ]
    }
   ],
   "source": [
    "training_acc = calc_accuracy(data.training_dataset.X, data.training_dataset.Y, mft)\n",
    "testing_acc = calc_accuracy(data.testing_dataset.X, data.testing_dataset.Y, mft)\n",
    "\n",
    "print(f'Training Accuracy: {training_acc*100:.2f}%')\n",
    "print(f'Testing Accuracy: {testing_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModelWrapper(object):\n",
    "    def __init__(self, dataset, model):\n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        \n",
    "        self.freq_counter = self._frequency_counter(self.dataset.Y, self.dataset.X)\n",
    "        self.unigram_table = self._get_unigram_count()\n",
    "        self.bigram_table = self._get_bigram_count()\n",
    "        self.starting_table = self._get_starting_sentence_count()\n",
    "        self.ending_table = self._get_ending_sentence_count()\n",
    "        \n",
    "        self._prepare_model()\n",
    "    \n",
    "    def _get_unigram_count(self):\n",
    "        flattened = []\n",
    "        for tags in self.dataset.Y:\n",
    "            flattened += tags\n",
    "        \n",
    "        return Counter(flattened)\n",
    "    \n",
    "    def _get_bigram_count(self):\n",
    "        pairs = []\n",
    "        for tags in self.dataset.Y:\n",
    "            for current_tag, next_tag in zip(tags[:-1], tags[1:]):\n",
    "                pairs.append((current_tag, next_tag))\n",
    "        \n",
    "        return Counter(pairs)\n",
    "    \n",
    "    def _get_starting_sentence_count(self):\n",
    "        starting_tags = []\n",
    "        for tags in self.dataset.Y:\n",
    "            starting_tags.append(tags[0])\n",
    "        return Counter(starting_tags)\n",
    "    \n",
    "    def _get_ending_sentence_count(self):\n",
    "        ending_tags = []\n",
    "        for tags in self.dataset.Y:\n",
    "            ending_tags.append(tags[-1])\n",
    "        return Counter(ending_tags)\n",
    "    \n",
    "    def _frequency_counter(self, seq_A, seq_B):\n",
    "        counter = defaultdict(Counter)\n",
    "        \n",
    "        for i in range(len(seq_A)):\n",
    "            for a, b in zip(seq_A[i], seq_B[i]):\n",
    "                counter[a][b] += 1\n",
    "                \n",
    "        return counter\n",
    "    \n",
    "    def _prepare_model(self):\n",
    "        states = {}\n",
    "        \n",
    "        for tag in self.dataset.tags:\n",
    "            emission_prob = {}\n",
    "            for word in self.freq_counter[tag]:\n",
    "                emission_prob[word] = self.freq_counter[tag][word] / self.unigram_table[tag]\n",
    "        \n",
    "            emission_per_tag = DiscreteDistribution(emission_prob)\n",
    "            tag_state = State(emission_per_tag, tag)\n",
    "            states[tag] = tag_state\n",
    "        \n",
    "        self.model.add_states([state for state in states.values()])\n",
    "        \n",
    "        for tag in self.dataset.tags:\n",
    "            start_prob = self.starting_table[tag] / self.starting_table.total()\n",
    "            self.model.add_transition(self.model.start, states[tag], start_prob)\n",
    "            \n",
    "        for tag in self.dataset.tags:\n",
    "            end_prob = self.ending_table[tag] / self.unigram_table[tag]\n",
    "            self.model.add_transition(states[tag], self.model.end, end_prob)\n",
    "            \n",
    "        for current_tag, next_tag in self.bigram_table.keys():\n",
    "            transition_prob = self.bigram_table[(current_tag, next_tag)] / self.unigram_table[current_tag]\n",
    "            self.model.add_transition(states[current_tag], states[next_tag], transition_prob)\n",
    "            \n",
    "        self.model.bake()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sentence = []\n",
    "        predictions = ['<start>']\n",
    "            \n",
    "        for word in X:\n",
    "            if word in self.dataset.vocab:\n",
    "                sentence.append(word)\n",
    "            else:\n",
    "                sentence.append('nan')\n",
    "        \n",
    "        _, state_path = self.model.viterbi(sentence)\n",
    "        for state in state_path[1:-1]:\n",
    "            predictions.append(state[1].name)\n",
    "        predictions.append('<end>')\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "hmm = HiddenMarkovModelWrapper(data.training_dataset, HiddenMarkovModel(name=\"base-hmm-tagger\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Sentence ID:  b100-49401\n",
      "Sentence:  ['``', \"i'll\", 'shore', 'be', 'needing', 'ye', 'both', 'on', 'the', 'pull', 'out', \"o'\", 'the', 'canyon', \"''\", '.']\n",
      "\n",
      "Prediction: \n",
      " ['<start>', '.', 'prt', 'noun', 'verb', 'verb', 'pron', 'det', 'adp', 'det', 'verb', 'prt', 'adp', 'det', 'noun', '.', '.', '<end>']\n",
      "Actual: \n",
      " ['<start>', '.', 'prt', 'noun', 'verb', 'verb', 'pron', 'det', 'adp', 'det', 'noun', 'prt', 'adp', 'det', 'noun', '.', '.', '<end>']\n",
      "---------------------------\n",
      "Sentence ID:  b100-18537\n",
      "Sentence:  ['she', 'named', '48', 'items', ',', 'and', 'said', 'there', 'were', '``', 'many', 'more', 'things', 'which', 'it', 'would', 'take', 'too', 'long', 'to', 'write', \"''\", '.']\n",
      "\n",
      "Prediction: \n",
      " ['<start>', 'pron', 'verb', 'num', 'noun', '.', 'conj', 'verb', 'prt', 'verb', '.', 'adj', 'adj', 'noun', 'det', 'pron', 'verb', 'verb', 'adv', 'adj', 'prt', 'verb', '.', '.', '<end>']\n",
      "Actual: \n",
      " ['<start>', 'pron', 'verb', 'num', 'noun', '.', 'conj', 'verb', 'prt', 'verb', '.', 'adj', 'adj', 'noun', 'det', 'pron', 'verb', 'verb', 'adv', 'adv', 'prt', 'verb', '.', '.', '<end>']\n",
      "---------------------------\n",
      "Sentence ID:  b100-35184\n",
      "Sentence:  [\"merton's\", 'functional', 'sociology', 'may', 'have', 'great', 'practical', 'use', 'in', 'the', 'study', 'of', 'different', 'cultures', ',', 'yet', 'it', 'is', 'perfectly', 'clear', ',', 'as', 'nagel', '(', '1957', ':', ':']\n",
      "\n",
      "Prediction: \n",
      " ['<start>', 'det', 'adj', 'noun', 'verb', 'verb', 'adj', 'adj', 'noun', 'adp', 'det', 'noun', 'adp', 'adj', 'noun', '.', 'conj', 'pron', 'verb', 'adv', 'adj', '.', 'adp', 'noun', '.', 'num', '.', '.', '<end>']\n",
      "Actual: \n",
      " ['<start>', 'noun', 'adj', 'noun', 'verb', 'verb', 'adj', 'adj', 'noun', 'adp', 'det', 'noun', 'adp', 'adj', 'noun', '.', 'conj', 'pron', 'verb', 'adv', 'adj', '.', 'adp', 'noun', '.', 'num', '.', '.', '<end>']\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "visualize_predictions(data.testing_dataset, hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 97.41%\n",
      "Testing Accuracy: 96.02%\n"
     ]
    }
   ],
   "source": [
    "training_acc = calc_accuracy(data.training_dataset.X, data.training_dataset.Y, hmm)\n",
    "testing_acc = calc_accuracy(data.testing_dataset.X, data.testing_dataset.Y, hmm)\n",
    "\n",
    "print(f'Training Accuracy: {training_acc*100:.2f}%')\n",
    "print(f'Testing Accuracy: {testing_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c322dea300a58550f3ae981d8c0bb7c2b8c63dddf722abf1209130a9e78caf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
